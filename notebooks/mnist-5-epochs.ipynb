{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54bab110-2a71-4d16-9f16-a7c0884a7019",
   "metadata": {},
   "source": [
    "# MNIST, 5 Epochs\n",
    "\n",
    "Create a Kangas DataGrid while training a tensorflow\n",
    "model on the MNIST dataset.\n",
    "\n",
    "To get started, we'll need to make sure we have at least these packages (in addition to tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b0bdfa-5dad-43e5-bbb3-69d02b532775",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kangas --upgrade --quiet\n",
    "%pip install aitk --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78db920-3d3c-431f-b0f5-19697b504578",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook will train a simple neural network\n",
    "for 5 epochs on the MNIST training data.\n",
    "\n",
    "The neural network takes an MNIST digit representation\n",
    "as input, and outputs 10 predictions (one for each\n",
    "digit). The output node with the highest output activation\n",
    "wins.\n",
    "\n",
    "The Kangas DataGrid logs all of the outputs for each\n",
    "input, including the input (logged as an image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "657b0ea6-e6b3-43b6-aeff-35eb5ea4594f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 09:49:47.679267: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-12 09:49:47.679304: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from kangas import DataGrid, Image\n",
    "from aitk.networks import Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1008a0-4c86-4a16-863e-554bfda2b4a5",
   "metadata": {},
   "source": [
    "For this simple model, we'll build a flat deep learning network. It won't use any convolution layers, but will treat the data as a simple 1D vector. This is not a good way to treat 2D data, but it makes for a simple start.\n",
    "\n",
    "We define a function that creates the model, given a set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fce64785-ff8a-4d29-aa27-eefd2833f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(parameters):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Dense(\n",
    "            parameters[\"layer1_size\"],\n",
    "            activation=\"sigmoid\",  # relu, sigmoid\n",
    "            name=\"hidden1\",\n",
    "            input_shape=(784,),\n",
    "        )\n",
    "    )\n",
    "    model.add(Dense(parameters[\"layer2_size\"], name=\"hidden2\", activation=\"sigmoid\"))\n",
    "    model.add(Dense(parameters[\"layer3_size\"], name=\"hidden3\", activation=\"sigmoid\"))\n",
    "    model.add(Dense(10, name=\"output\", activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=RMSprop(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3126f0a-dae8-44c9-8847-c53c217d204a",
   "metadata": {},
   "source": [
    "We create a model, with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a12d6cc-03e2-43eb-9350-ec5df7ae6877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 09:49:50.928383: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-12 09:49:50.928541: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-12 09:49:50.928649: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (saliva3): /proc/driver/nvidia/version does not exist\n",
      "2023-04-12 09:49:50.929635: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"layer1_size\": 50,\n",
    "    \"layer2_size\": 25,\n",
    "    \"layer3_size\": 25,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 5,\n",
    "}\n",
    "model = build_model(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ed3043-2d25-4019-a816-2af195da674e",
   "metadata": {},
   "source": [
    "We create a function to download and format the inputs and targets as we desire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9281473-d9f0-435a-a0f9-5f2eb2a05391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    num_classes = 10\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape(60000, 784)\n",
    "    x_test = x_test.reshape(10000, 784)\n",
    "    x_train = x_train.astype(\"float32\")\n",
    "    x_test = x_test.astype(\"float32\")\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    print(x_train.shape[0], \"train samples\")\n",
    "    print(x_test.shape[0], \"test samples\")\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = to_categorical(y_train, num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8b1ba44-1239-4dd7-89f0-6ca938054e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = get_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f587913e-4843-4dd4-929f-b5b125156954",
   "metadata": {},
   "source": [
    "For visualization purposes, we use `aitk.network.Network`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67c99ecb-a989-4714-bccd-b6a74a195b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae95cf6b-e51d-4f87-ae31-216102b2b90e",
   "metadata": {},
   "source": [
    "To see what the network structure, and input representations, look like, we give `network.display()` an input representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b32d79a-0430-4ddf-8ce0-204a39ab65e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg id='keras-network' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink' image-rendering=\"pixelated\" width=\"380px\" height=\"400px\" style=\"background-color: #B0C4DE\">\n",
       " <g >\n",
       "  <svg viewBox=\"0 0 400 420\" width=\"380px\" height=\"400px\">\n",
       "    <defs>\n",
       "        <marker id=\"arrow\" markerWidth=\"10\" markerHeight=\"10\" refX=\"9\" refY=\"3\" orient=\"auto\" markerUnits=\"strokeWidth\">\n",
       "          <path d=\"M0,0 L0,6 L9,3 z\" fill=\"black\" />\n",
       "        </marker>\n",
       "    </defs><rect x=\"99.0\" y=\"24\" width=\"202\" height=\"52\" style=\"fill:none;stroke:black;stroke-width:2\"/><image id=\"keras-network_output\" class=\"keras-network\" x=\"100.0\" y=\"25\" height=\"50\" width=\"200\" preserveAspectRatio=\"none\" image-rendering=\"optimizeSpeed\" xlink:href=\"data:image/gif;base64,R0lGODdhCgABAIMAAAgICA0NDRERERUVFRYWFh4eHiAgICsrKy4uLgAAAAAAAAAAAAAAAAAAAAAAAAAAACwAAAAACgABAAAIDgAPCBBQgICBAQEAIAgIADs=\"><title>Layer: output 'Dense'\n",
       "Act function: softmax\n",
       "Act output range: (0.0, 1.0)\n",
       "Actual minmax: (0.0, 1.0)\n",
       "Shape = (None, 10)</title></image><text x=\"305.0\" y=\"52.0\" font-family=\"monospace\" font-size=\"12\" text-anchor=\"start\" fill=\"blue\" alignment-baseline=\"central\" >output</text><path d=\"M 200.0 104 L 200.0 77 \" stroke=\"black\" stroke-width=\"2\" marker-end=\"url(#arrow)\" fill=\"none\" /><rect x=\"99.0\" y=\"104\" width=\"202\" height=\"52\" style=\"fill:none;stroke:black;stroke-width:2\"/><image id=\"keras-network_hidden3\" class=\"keras-network\" x=\"100.0\" y=\"105\" height=\"50\" width=\"200\" preserveAspectRatio=\"none\" image-rendering=\"optimizeSpeed\" xlink:href=\"data:image/gif;base64,R0lGODdhGQABAIQAAENDQ1FRUVNTU1VVVWFhYWRkZGhoaGxsbG1tbXNzc3R0dHx8fH9/f4ODg4WFhYiIiIqKipSUlJWVlZaWlp2dnaCgoKGhoaioqAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwAAAAAGQABAAAIHwAdCEDgQMKDCQksUBgA4IIBAgwaRFhQQAGEAxUCBAQAOw==\"><title>Layer: hidden3 'Dense'\n",
       "Act function: sigmoid\n",
       "Act output range: (0.0, 1.0)\n",
       "Actual minmax: (0.0, 1.0)\n",
       "Shape = (None, 25)</title></image><text x=\"305.0\" y=\"132.0\" font-family=\"monospace\" font-size=\"12\" text-anchor=\"start\" fill=\"blue\" alignment-baseline=\"central\" >hidden3</text><path d=\"M 200.0 184 L 200.0 157 \" stroke=\"black\" stroke-width=\"2\" marker-end=\"url(#arrow)\" fill=\"none\" /><rect x=\"99.0\" y=\"184\" width=\"202\" height=\"52\" style=\"fill:none;stroke:black;stroke-width:2\"/><image id=\"keras-network_hidden2\" class=\"keras-network\" x=\"100.0\" y=\"185\" height=\"50\" width=\"200\" preserveAspectRatio=\"none\" image-rendering=\"optimizeSpeed\" xlink:href=\"data:image/gif;base64,R0lGODdhGQABAIQAACgoKDExMTQ0NExMTGRkZGVlZWZmZmlpaWtra2xsbHFxcXZ2dnh4eHp6eoCAgIyMjI6OjpOTk6Ojo6ioqKmpqbOzs7W1tQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwAAAAAGQABAAAIHwANPKigAIKAAwwcSGgwgIKFCQ4iJFgQgAAABAUOBAQAOw==\"><title>Layer: hidden2 'Dense'\n",
       "Act function: sigmoid\n",
       "Act output range: (0.0, 1.0)\n",
       "Actual minmax: (0.0, 1.0)\n",
       "Shape = (None, 25)</title></image><text x=\"305.0\" y=\"212.0\" font-family=\"monospace\" font-size=\"12\" text-anchor=\"start\" fill=\"blue\" alignment-baseline=\"central\" >hidden2</text><path d=\"M 200.0 264 L 200.0 237 \" stroke=\"black\" stroke-width=\"2\" marker-end=\"url(#arrow)\" fill=\"none\" /><rect x=\"99.0\" y=\"264\" width=\"202\" height=\"52\" style=\"fill:none;stroke:black;stroke-width:2\"/><image id=\"keras-network_hidden1\" class=\"keras-network\" x=\"100.0\" y=\"265\" height=\"50\" width=\"200\" preserveAspectRatio=\"none\" image-rendering=\"optimizeSpeed\" xlink:href=\"data:image/gif;base64,R0lGODdhMgABAIUAAD8/P0FBQVFRUVJSUlVVVVlZWVtbW15eXl9fX2VlZWZmZmdnZ25ubnBwcHFxcXZ2dnl5eXt7e319fYGBgYKCgoWFhYqKioyMjI+Pj5aWlpeXl5iYmJubm5+fn6Wlpaenp6mpqaysrMLCwgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwAAAAAMgABAAAIOgAHXMigAQIABx5AQJjQIEIIARsgWPiggUGHDRUUiHBQgIIBAhgyeAApIcACBxwIJDjAIcMDBRAQBAQAOw==\"><title>Layer: hidden1 'Dense'\n",
       "Act function: sigmoid\n",
       "Act output range: (0.0, 1.0)\n",
       "Actual minmax: (0.0, 1.0)\n",
       "Shape = (None, 50)</title></image><text x=\"305.0\" y=\"292.0\" font-family=\"monospace\" font-size=\"12\" text-anchor=\"start\" fill=\"blue\" alignment-baseline=\"central\" >hidden1</text><path d=\"M 200.0 344 L 200.0 317 \" stroke=\"black\" stroke-width=\"2\" marker-end=\"url(#arrow)\" fill=\"none\" /><rect x=\"99.00000000000001\" y=\"344\" width=\"201.99999999999997\" height=\"52\" style=\"fill:none;stroke:black;stroke-width:2\"/><image id=\"keras-network_hidden1_input\" class=\"keras-network\" x=\"100.00000000000001\" y=\"345\" height=\"50\" width=\"199.99999999999997\" preserveAspectRatio=\"none\" image-rendering=\"optimizeSpeed\" xlink:href=\"data:image/gif;base64,R0lGODdhEAMBAIYAAAAAAAEBAQICAgMDAwkJCQsLCw4ODhAQEBISEhcXFxgYGBkZGRoaGhsbGx4eHiMjIyQkJCcnJysrKy0tLS4uLjExMTc3Nzg4OEBAQEJCQkZGRk5OTlBQUFFRUVJSUlpaWl1dXV5eXmtra2xsbHJycnd3d35+fn9/f4KCgoSEhIWFhYeHh4iIiIuLi5SUlJaWlpqampycnKCgoKampqqqqqurq6ysrK+vr7a2tre3t7q6uru7u76+vsPDw8bGxsnJyc3Nzc/Pz9TU1NXV1dvb293d3eHh4eLi4uXl5e7u7vDw8PHx8fLy8vT09Pf39/n5+fr6+vv7+/z8/P39/f///wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwAAAAAEAMBAAAIzwABCBxIsKDBgwgTKlzIsKHDhxAhDkBA0QSLGwxmUHFygqEDCCFg0JhCkqQRG1OY9MCwsEKSkjCnRAHhwcOFCAwREInpA4eTJREBcIghgiSQAgAkwAgKwEAAGFM+MDXYYgoPAVMJFuAxRUNWgg+WGJExIsBXAB2UkCyx4OwEHSRfNDh7AISUKTvOCnwy5QnLqRRQ5CAZBGvQCC6QlISCI6gCEkVg/tgQMUGGITB9dDDsEEGNnSV7cCAA0YKNIzCbqEAakUVJIStSHNBLuzbtgAA7\"><title>Layer: hidden1_input 'InputLayer'\n",
       "Actual minmax: (0.0, 1.0)\n",
       "Shape = [(None, 784)]</title></image><text x=\"305.0\" y=\"372.0\" font-family=\"monospace\" font-size=\"12\" text-anchor=\"start\" fill=\"blue\" alignment-baseline=\"central\" >hidden1_input</text><text x=\"200.0\" y=\"12.5\" font-family=\"monospace\" font-size=\"15\" text-anchor=\"middle\" fill=\"black\" alignment-baseline=\"central\" >Activations for sequential</text></svg></g></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "network.display(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d7dd2-16d6-4c30-b00c-c35db189eda2",
   "metadata": {},
   "source": [
    "This is an accurate representation of the network. However, it makes the input vector a bit hard for humans to understand.\n",
    "\n",
    "What shape does an input representation have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76c6485f-cdc7-47de-af6c-7dbfd6ed4099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d7d957-8630-4a4d-985f-50c44f8d88c7",
   "metadata": {},
   "source": [
    "Ah, yes. I believe that 784 is the width times the height of the raw images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da447b6d-032d-41d4-a087-fbdbd65d9084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28 * 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b471ac30-0432-44fd-b37c-0d508b1d3cbd",
   "metadata": {},
   "source": [
    "Right. SO, let's set the visual representation for the humans among us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "715077a0-4969-46d9-a93a-cdeb9b84aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.set_config_layer(\"hidden1_input\", vshape=(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d7b439-20c3-42f2-a305-bb89359de960",
   "metadata": {},
   "source": [
    "And try displaying again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "500c6da0-8efe-4ca8-aac9-67b2f1444510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg id='keras-network' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink' image-rendering=\"pixelated\" width=\"280px\" height=\"400px\" style=\"background-color: #B0C4DE\">\n",
       " <g >\n",
       "  <svg viewBox=\"0 0 400 570\" width=\"280px\" height=\"400px\">\n",
       "    <defs>\n",
       "        <marker id=\"arrow\" markerWidth=\"10\" markerHeight=\"10\" refX=\"9\" refY=\"3\" orient=\"auto\" markerUnits=\"strokeWidth\">\n",
       "          <path d=\"M0,0 L0,6 L9,3 z\" fill=\"black\" />\n",
       "        </marker>\n",
       "    </defs><rect x=\"99.0\" y=\"24\" width=\"202\" height=\"52\" style=\"fill:none;stroke:black;stroke-width:2\"/><image id=\"keras-network_output\" class=\"keras-network\" x=\"100.0\" y=\"25\" height=\"50\" width=\"200\" preserveAspectRatio=\"none\" image-rendering=\"optimizeSpeed\" xlink:href=\"data:image/gif;base64,R0lGODdhCgABAIIAAAgICA0NDRERERUVFRYWFh8fHywsLC4uLiwAAAAACgABAAAIDgANCBBQgECBAQEAHAgIADs=\"><title>Layer: output 'Dense'\n",
       "Act function: softmax\n",
       "Act output range: (0.0, 1.0)\n",
       "Actual minmax: (0.0, 1.0)\n",
       "Shape = (None, 10)</title></image><text x=\"305.0\" y=\"52.0\" font-family=\"monospace\" font-size=\"12\" text-anchor=\"start\" fill=\"blue\" alignment-baseline=\"central\" >output</text><path d=\"M 200.0 104 L 200.0 77 \" stroke=\"black\" stroke-width=\"2\" marker-end=\"url(#arrow)\" fill=\"none\" /><rect x=\"99.0\" y=\"104\" width=\"202\" height=\"52\" style=\"fill:none;stroke:black;stroke-width:2\"/><image id=\"keras-network_hidden3\" class=\"keras-network\" x=\"100.0\" y=\"105\" height=\"50\" width=\"200\" preserveAspectRatio=\"none\" image-rendering=\"optimizeSpeed\" xlink:href=\"data:image/gif;base64,R0lGODdhGQABAIQAAEVFRVBQUFJSUlRUVGFhYWVlZWlpaW1tbXJycnR0dHp6en9/f4GBgYODg4eHh4iIiImJiZSUlJaWlp6enqGhoaqqqgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwAAAAAGQABAAAIHwAdCDjQQMIDCQgoTBgAoIIBAgsYRFBQIAGEAxQCBAQAOw==\"><title>Layer: hidden3 'Dense'\n",
       "Act function: sigmoid\n",
       "Act output range: (0.0, 1.0)\n",
       "Actual minmax: (0.0, 1.0)\n",
       "Shape = (None, 25)</title></image><text x=\"305.0\" y=\"132.0\" font-family=\"monospace\" font-size=\"12\" text-anchor=\"start\" fill=\"blue\" alignment-baseline=\"central\" >hidden3</text><path d=\"M 200.0 184 L 200.0 157 \" stroke=\"black\" stroke-width=\"2\" marker-end=\"url(#arrow)\" fill=\"none\" /><rect x=\"99.0\" y=\"184\" width=\"202\" height=\"52\" style=\"fill:none;stroke:black;stroke-width:2\"/><image id=\"keras-network_hidden2\" class=\"keras-network\" x=\"100.0\" y=\"185\" height=\"50\" width=\"200\" preserveAspectRatio=\"none\" image-rendering=\"optimizeSpeed\" xlink:href=\"data:image/gif;base64,R0lGODdhGQABAIQAAC8vLzQ0NDk5OUxMTFlZWV1dXWFhYWZmZmlpaW1tbXd3d3t7e319fYCAgIKCgoeHh5CQkJubm6ampqysrK2trbS0tLW1tQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwAAAAAGQABAAAIHwALPKiAAIKAAwoaUHAwYIIFCQsiIGAQgACABAYSBAQAOw==\"><title>Layer: hidden2 'Dense'\n",
       "Act function: sigmoid\n",
       "Act output range: (0.0, 1.0)\n",
       "Actual minmax: (0.0, 1.0)\n",
       "Shape = (None, 25)</title></image><text x=\"305.0\" y=\"212.0\" font-family=\"monospace\" font-size=\"12\" text-anchor=\"start\" fill=\"blue\" alignment-baseline=\"central\" >hidden2</text><path d=\"M 200.0 264 L 200.0 237 \" stroke=\"black\" stroke-width=\"2\" marker-end=\"url(#arrow)\" fill=\"none\" /><rect x=\"99.0\" y=\"264\" width=\"202\" height=\"52\" style=\"fill:none;stroke:black;stroke-width:2\"/><image id=\"keras-network_hidden1\" class=\"keras-network\" x=\"100.0\" y=\"265\" height=\"50\" width=\"200\" preserveAspectRatio=\"none\" image-rendering=\"optimizeSpeed\" xlink:href=\"data:image/gif;base64,R0lGODdhMgABAIUAAD8/P0RERE5OTk9PT1VVVVdXV1hYWF1dXWBgYGFhYWdnZ2hoaGxsbG5ubnFxcXV1dXl5eX9/f4KCgoWFhYaGhoiIiIqKiouLi4+Pj5OTk5WVlZaWlpeXl5iYmJmZmZqampubm56enqKioqOjo6SkpKWlpampqaurq7Ozs7S0tLa2tgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwAAAAAMgABAAAIOwARnAChAUOCCAQ6PIAgYYEHASMyUDBBIkAJAxUiqPjgwIKDBgcyiCjQAcMABhRQGOCgIMSGFAAmXAgIADs=\"><title>Layer: hidden1 'Dense'\n",
       "Act function: sigmoid\n",
       "Act output range: (0.0, 1.0)\n",
       "Actual minmax: (0.0, 1.0)\n",
       "Shape = (None, 50)</title></image><text x=\"305.0\" y=\"292.0\" font-family=\"monospace\" font-size=\"12\" text-anchor=\"start\" fill=\"blue\" alignment-baseline=\"central\" >hidden1</text><path d=\"M 200.0 344 L 200.0 317 \" stroke=\"black\" stroke-width=\"2\" marker-end=\"url(#arrow)\" fill=\"none\" /><rect x=\"99.0\" y=\"344\" width=\"202\" height=\"202\" style=\"fill:none;stroke:black;stroke-width:2\"/><image id=\"keras-network_hidden1_input\" class=\"keras-network\" x=\"100.0\" y=\"345\" height=\"200\" width=\"200\" preserveAspectRatio=\"none\" image-rendering=\"optimizeSpeed\" xlink:href=\"data:image/gif;base64,R0lGODdhHAAcAIYAAAAAAAYGBgcHBwoKCgwMDBMTExUVFRkZGRwcHB0dHSUlJSYmJi8vLzAwMDIyMjMzMzY2Njg4ODk5OTw8PD8/P0dHR0tLS0xMTE9PT1RUVFVVVVZWVmBgYHBwcHJycnl5eXp6eoCAgIKCgoODg4eHh42NjZGRkZKSkpSUlJ+fn6KioqOjo6Wlpaenp6ioqK2trbKysrOzs7q6ur29vb6+vsPDw8TExMbGxsfHx8rKytDQ0NfX19/f3+Dg4OHh4ePj4+Tk5OXl5ebm5unp6e3t7e7u7u/v7/Dw8PPz8/b29vn5+fz8/P39/f///wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwAAAAAHAAcAEAI5gABCBxIsKDBgwgHrljCkAlDDjOYtEg4UMMSIQcQCiDBRAYBiiBDihT4IAWTFA5GAhAAY8mRCgUQGHRYA6RFJScaJNAAwwcTHi0iqBxKVCCEHw6NDFkiIYDKBkUYxiBg4YMBgUyQpAR5gwkNik2Y2LC5xMTBBiyWvBiJAEdDhkPMFp1Lt65dilAZLiEyd8CEHksc5sgQGMTQB0WYePXABAgDDE1cjFzAgskQHRlmLmERUgJDCglpgryQZEkHig5RkOWhccQSHxVCbmDiw6CHxCpUWmQYZIdeGyKKHggReEkJBXeT3w0IADs=\"><title>Layer: hidden1_input 'InputLayer'\n",
       "Actual minmax: (0.0, 1.0)\n",
       "Shape = [(None, 784)]</title></image><text x=\"305.0\" y=\"447.0\" font-family=\"monospace\" font-size=\"12\" text-anchor=\"start\" fill=\"blue\" alignment-baseline=\"central\" >hidden1_input</text><text x=\"200.0\" y=\"12.5\" font-family=\"monospace\" font-size=\"15\" text-anchor=\"middle\" fill=\"black\" alignment-baseline=\"central\" >Activations for sequential</text></svg></g></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "network.display(x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611457b3-f955-4fb0-bee5-fa2828314764",
   "metadata": {},
   "source": [
    "What do we see?\n",
    "\n",
    "1. The input layer is the MNIST digit image data\n",
    "2. The network's output layer has 10 units, one for each digit\n",
    "3. The activations flow from from bottom to top\n",
    "4. Before training, the output is basically random, because the weights are random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f16443a-1dcb-4bf3-9ba7-87bdce46a67d",
   "metadata": {},
   "source": [
    "For this experiment, we'll make a Kangas DataGrid to keep track of each test input digit (\"Image), what the proper category is, (\"Truth\"), what the network thinks it is (\"Output\", the max of the following), and the output of each unit on the ouptut layer (\"score_0\" through \"score_9\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6f8726e-b94a-40db-9968-f8f746f8ecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = DataGrid(\n",
    "    name=\"mnist-5-epochs\",\n",
    "    columns=[\n",
    "        \"Epoch\",\n",
    "        \"Index\",\n",
    "        \"Image\",\n",
    "        \"Truth\",\n",
    "        \"Output\",\n",
    "        \"score_0\",\n",
    "        \"score_1\",\n",
    "        \"score_2\",\n",
    "        \"score_3\",\n",
    "        \"score_4\",\n",
    "        \"score_5\",\n",
    "        \"score_6\",\n",
    "        \"score_7\",\n",
    "        \"score_8\",\n",
    "        \"score_9\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c955b97e-ac49-4692-8676-624ae67a600d",
   "metadata": {},
   "source": [
    "First, we make image of the test corpus, to reuse them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9635c5cc-39f9-4dcf-99a5-ae922291eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [Image(test, shape=(28, 28)) for test in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6b5317-5a4e-4afa-91cc-85cd0d1e617d",
   "metadata": {},
   "source": [
    "We run the test data through the model once before training as a baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99923850-33b4-424a-a6b6-a9144ea3821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.predict(x_test)\n",
    "epoch = 0\n",
    "for index in range(len(x_test)):\n",
    "    truth = int(y_test[index].argmax())\n",
    "    guess = int(outputs[index].argmax())\n",
    "    dg.append([epoch, index, images[index], truth, guess] + list(outputs[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b120ad61-30b6-4e53-a3b1-54480c97d763",
   "metadata": {},
   "source": [
    "And define a function that will train the training data for one epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e668549-93c4-4171-982f-aa0426a05bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(parameters, model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=parameters[\"batch_size\"],\n",
    "        epochs=1,\n",
    "        validation_data=(x_test, y_test),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2bd9a4-4a20-430d-ad45-8184145717e5",
   "metadata": {},
   "source": [
    "And we are ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8512bdc4-00ec-4940-aea4-0d3cc78f3099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 09:49:58.777091: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.2609 - accuracy: 0.6720 - val_loss: 0.6033 - val_accuracy: 0.8639\n",
      " 96/938 [==>...........................] - ETA: 1s - loss: 0.5775 - accuracy: 0.8657"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 09:50:02.630636: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 0.4262 - accuracy: 0.8947 - val_loss: 0.3236 - val_accuracy: 0.9181\n",
      " 91/938 [=>............................] - ETA: 1s - loss: 0.3205 - accuracy: 0.9178"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 09:50:05.966214: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2770 - accuracy: 0.9271 - val_loss: 0.2422 - val_accuracy: 0.9364\n",
      " 88/938 [=>............................] - ETA: 1s - loss: 0.2305 - accuracy: 0.9389"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 09:50:09.145775: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2176 - accuracy: 0.9413 - val_loss: 0.2029 - val_accuracy: 0.9450\n",
      " 60/938 [>.............................] - ETA: 1s - loss: 0.1663 - accuracy: 0.9542"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 09:50:12.611366: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1818 - accuracy: 0.9510 - val_loss: 0.1824 - val_accuracy: 0.9494\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, parameters[\"epochs\"] + 1):\n",
    "    train(parameters, model, x_train, y_train, x_test, y_test)\n",
    "    outputs = model.predict(x_test)\n",
    "\n",
    "    for index in range(len(x_test)):\n",
    "        truth = int(y_test[index].argmax())\n",
    "        guess = int(outputs[index].argmax())\n",
    "        dg.append(\n",
    "            [epoch, index, images[index], truth, guess] + list(outputs[index])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ed75a-e647-4534-a1e9-c79fd5e30ec7",
   "metadata": {},
   "source": [
    "We save the DataGrid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "361c423f-8329-438d-8e5d-2e070274d4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60000/60000 [00:01<00:00, 33781.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving datagrid to 'mnist-5-epochs.datagrid'...\n",
      "Extending data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60000/60000 [00:05<00:00, 10123.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.20it/s]\n"
     ]
    }
   ],
   "source": [
    "dg.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff816af0-fbee-47c0-8a80-75e3d2c8e679",
   "metadata": {},
   "source": [
    "And now we are ready to see the DataGrid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7294c50-ce1b-4f24-8c18-eab13fd31a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"750px\"\n",
       "            src=\"http://127.0.1.1:4000/?datagrid=mnist-5-epochs.datagrid\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fba17e63df0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f780b3-9950-4a2f-83f3-6dd6a7ae2f7a",
   "metadata": {},
   "source": [
    "Some example filters to try in the UI:\n",
    "\n",
    "1. Show rows before training: `{\"epoch\"} == 0`\n",
    "2. Show rows after training: `{\"epoch\"} == 5`\n",
    "3. Show rows after training, for correct outputs: `{\"epoch\"} == 5 and {\"output\"} == {\"guess\"}`\n",
    "4. Show rows after training, for incorrect outputs: `{\"epoch\"} == 5 and {\"output\"} != {\"guess\"}`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
